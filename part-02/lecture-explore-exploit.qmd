---
title: "Lecture II - Explore Vs. Exploit"
subtitle: "Programming: Everyday Decision-Making Algorithms"
author: "Dr. Nils Roemer"
institute: "Kühne Logistics University Hamburg - Winter 2024"
title-slide-attributes:
    data-background-color: "#8C3048"

format:
    revealjs:
        theme: [default, ../styles.scss]
        transition: slide
        transition-speed: fast
        highlight-style: arrow
        code-overflow: wrap
        slide-number: true
        code-copy: true
        height: 750
        footer: " {{< meta title >}} | {{< meta author >}} | [Home](lecture-optimal-stopping.qmd)"
        output-file: lecture-presentation.html
    html:
        theme: [litera, ../styles_html.scss]
    pdf: 
        documentclass: report

execute:
    freeze: auto
    echo: true
---

# [Explore Vs. Exploit]{.flow} {.title}

## Some definitions...

1) [Question:]{.question} What does "explore" mean?
2) [Question:]{.question} What does "exploit" mean?
3) [Question:]{.question} what is the relationship between ‘explore’ and ‘exploit’ ?

:::{.incremental}
1) [Explore]{.highlight} is the gathering of new information
2) [Exploit]{.highlight} exploit is the utilisation of already known information in order to obtain a known result.
3) [Explore and exploit]{.highlight} are opposing action alternatives in a trade-off.
:::

## Some examples...

[Your stomach rumbles...]{.highlight}
    - Do you go to the italian restaurant, that you know and love?
    - Or do you try the new restaurant down the road, that you have never been to before?
    - Do you take your best friend with you?
    - Or are you asking a new acquaintance that you would like to get to know better?
[This is too hard, maybe you'll just stay at home and cook]{.highlight}
    - Do you cook your favorite meal?
    - Or do you try a new recipe from the internet that you have never cooked before?
    - ...

## Everyday decision-making

- Explore: Do we try new things?
- Exploit: Do we stick to our favorite ones?
- Live is a trade-off, a balance:
    - between novelty and tradition.
    - between the latest and the greatest.
    - between explore and exploit.
- [Question:]{.question} What is the optimal balance?
- Computer scientists have been working on this problem for over 50 years.

## [Question:]{.question} What is the problem with exploitation?

- Exploitation is not always the best strategy.
- Especially when you have very limited information.
- When you stop exploring, you might miss out on better options.
- Imagine you were no longer able to gather any new information and could only choose between known alternatives. 

## [Question:]{.question} What is the problem with exploration?

- Exploration is not always the best strategy.
- Especially when you are limited to use the new gathered information.
- When you constantly explore, you might never enjoy the fruits of your exploration.
- Imagine you could eat each meal only once, hear each song only once, talk to each person only once...

## To provide decision support, computer scientists formulated the explore-exploit trade-off as the multi-armed bandit problem.

[Question:]{.question} What is a one-armed bandit?

## [One Armed Bandit]{.invert-font} {background-image="https://images.unsplash.com/photo-1578693988706-534015857ff5?q=80&w=2664&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"}
::: footer
[Photo by <a href="https://unsplash.com/de/@evoknow">Kabir M</a> on Unsplash]{.invert-font}
:::

## The multi-armed bandit problem I

- A gambler is faced with a room of slot machines (one-armed bandits).
- Each slot machine has a different probability of winning.
- [Question:]{.question} What does the scenario have to do with explore vs exploit?

## The multi-armed bandit problem II

- By playing a slot machine, the gambler can gather information about the probability of winning.
- But each pull of a lever comes with a certain cost.
- It's the aim of the gambler to maximise his winnings.
- [Question:]{.question} What is the optimal strategy?

## The multi-armed bandit problem III

- Consider the following scenario:
    - You already pulled the lever of two machines.
    - Machine A: 15 pulls, 9 wins.
    - Machine B: 2 pulls, 1 win.
- [Question:]{.question} Which machine should you play next?

## Expected value as a decision criterion

- The expected value of a slot machine is the average number of wins per pull.
- Machine A: 9/15 = 0.6
- Machine B: 1/2 = 0.5
- Machine A has the higher expected value.
- But 2 and even 15 pulls are not a large number (considering standard deviation).

## The multi-armed bandit problem IV

- The multi-armed bandit problem represents a lot of different real-world decisions.
- It shows us, that there might be a difference between the optimal long-term average performance and the optimal immediate performance.
- Which lever to pull next depends completely on something we havent discussed yet: 

## The multi-armed bandit problem V
- How long you plan to be in the casino?
- [Question:]{.question} why is this important?
- "I'm more likely to try a new restaurant when I move to a city than when I'm leaving it" (Chris Stucchio)

## The influence of the interval

- Let's call the time you plan to be in the casino "the interval".
- The longer the interval, the more (in general) you should explore, since you will have more opportunities to exploit the gathered information.
- The shorter the interval, the more you should exploit your current knowledge.
- The optimal strategy depends on the length of the interval.

## The interval makes the strategy

- Explore when you have the time to use the resulting knowledge.
- "I moved to Pune, India, and I just [...] eat everywhere that didn't look like it was gonna kill me" (Chris Stucchio)
- Exploit when you are ready to cash in.
- "And as I was leaving the city I went back to all my old favorites, rather than trying out new stuff [...]. Even if I find a slightly better place, I'm only going to go there once or twice, so why take the risk?" (Chris Stucchio)

## Reality check: Explore vs Exploit.

## [Exploration]{.invert-font} {background-image="https://images.unsplash.com/photo-1491013516836-7db643ee125a?q=80&w=2650&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"}
::: footer
[Photo by <a href="https://unsplash.com/de/@invent">Colin Maynard</a> on Unsplash]{.invert-font}
:::

## [Exploitation]{.invert-font} {background-image="https://images.unsplash.com/photo-1504004030892-d06adf9ffbcf?q=80&w=2670&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"}

::: footer
[Photo by <a href="https://unsplash.com/de/@cristina_gottardi">Cristina Gottardi</a> on Unsplash]{.invert-font}
:::

---

## Reverse Engineering: Derivation of the interval by observing the strategy (excursion)

- Among the the ten highest-grossing movies, how many were sequels?
    - 1981: 2
    - 1991: 3
    - 2001: 5
    - 2011: 8
- [Question:]{.question} Do you have an explanation for the observed trend?

## [Sequels...]{.invert-font} {background-image="https://upload.wikimedia.org/wikipedia/commons/3/3d/Fast_X_logo.png" background-size="contain" background-position="center" background-repeat="no-repeat"}

::: footer
[Photo by <a href="https://de.wikipedia.org/wiki/Fast_%26_Furious_10">Universal Pictures</a> on Wikipedia]{.invert-font}
:::

## Reverse Engineering: A possible explanation

- Making a brand new movie is risky but has the potential to create a new fan base. (explore)
- From a Studio's perspective, a sequel is a movie with a guaranteed fan base, a cash cow, a sure thing, an exploit.
- One possible explanation for the numbers is that the studios think they are approaching the end of their interval. 
- They are pulling the arms of the best machines they've got before the casino turns them out.

## The multi-armed bandit problem VI

- While the so far provided anecdotes are helping us to understand the explore-exploit trade-off, they are far away from beeing a satisfying "optimal" solution.
- Actually, finding an algorithm that tells us exactly how to to handle the trade-off is a very hard problem.
- On the way there were many interesting approaches...

## Win-Stay Lose-Shift[^1]

- The win-stay lose-shift strategy is a simple strategy that is often used in multi-armed bandit problems.
- It is based on the idea that if you have won, you should stay with the same machine.
- If you have lost, you should switch to a different machine.
- This strategy is not always the best strategy, but it is simple and proven better than choose an arm at random.
- [Question:]{.question} How would you test this proof using Python?

...

[^1]: For more details see Robbins, H. (1952) ‘Some aspects of the sequential design of experiments’, Bulletin of the American Mathematical Society, 58. 

## Win-Stay is a no brainer

- If you decide to pull an arm and you win, you should pull the same arm again.
- Nothing changes, except the attractiveness of the arm you just pulled is higher.

## Lose-Shift is another story

- Changing arms each time you lose might be a prety rush move.
- Imagine you're eating at your favorite restaurant for the tenth time in a row. 
- You have always been very satisfied (win), but today you are disappointed (loose). 
- Should you turn your back on the restaurant?

## Like most of the time, easy answers comes with problems

- The win-stay lose-shift strategy penalizes loses to much.
- The strategy does not take into account the interval.
- But the strategy laid the foundation for better approaches

## Win-Stay Lose-Shift reloaded I

- Few years later, Richard Bellmann, found an exact solution to the problem for all cases with finite and known intervals.
- The solution is based on dynamic programming and backward calculation starting from the final pull (analogous to the secretary problem with full-information).

## Win-Stay Lose-Shift reloaded II

- The calculation of the optimal strategy are very extensive.
- And yet the approach does not help us in many scenarios because we do not know the exact number of opportunities  / time in the casino (length of the interval).
- At this point, it looked like the multi-armed bandit problem would remain a problem without a solution.

## The Gittins Index I[^2]

- In the 1970s, the Unilever Corporation asked a young mathematician, John Gittins, to help optimize their drug trials.
- Given several different chemical compounds, what is the quickest way to determine which compound is likely to be effective against a disease?
- Gittins found an optimal strategy, abstracted the problem to a general level, and realized that what he had discovered was, in fact, the solution to the multi-armed bandit problem.

[^2]: Gittins, J. (1979) ‘Bandit Processes and Dynamic Allocation Indices’, Journal of the Royal Statistical Society. Series B (Methodological). 41 (2).

## The Gittins Index II

- A major problem with the multi-armed banded problem is that previous solutions made very critical assumptions about the underlying interval.
- For example, that the length of the interval is known at the beginning of the analysis.
- Gittins developed a charming solution to this problem. In his approach, future wins (e.g. cash flows) are discounted[^3] so that any interval length (including infinity) can be considered. 

...

[^3]: Gittins makes a geometric discounting assumption, but the approach can be extended to other discounting assumptions.

## Reality check: Discounting

- Does discounting future wins make sense?
- Regarding monetary wins, it does. For example due to interest rates and oppurtunity costs.
- Regarding non-monetary wins, it is more difficult to justify.
- But its not counterintuitive.
- What is more important to you today, tonight's dinner, or ceteris paribus the dinner in a week's time?

## The Gittins Index III

- The Gittins index can be used to any problems of the form of the multi-armed bandit problem.
- That means it solves the explore-exploit trade-off.
- It is optimal.
